{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "from time import time\n",
    "\n",
    "import warc\n",
    "from bs4 import BeautifulSoup\n",
    "from selectolax.parser import HTMLParser\n",
    "\n",
    "\n",
    "def get_text_bs(html):\n",
    "    tree = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    body = tree.body\n",
    "    if body is None:\n",
    "        return None\n",
    "\n",
    "    for tag in body.select('script'):\n",
    "        tag.decompose()\n",
    "    for tag in body.select('style'):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = body.get_text(separator='\\n')\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_text_selectolax(html):\n",
    "    tree = HTMLParser(html)\n",
    "\n",
    "    if tree.body is None:\n",
    "        return None\n",
    "\n",
    "    for tag in tree.css('script'):\n",
    "        tag.decompose()\n",
    "    for tag in tree.css('style'):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = tree.body.text(separator='\\n')\n",
    "    return text\n",
    "\n",
    "\n",
    "def read_doc(record, parser=get_text_selectolax):\n",
    "    url = record.url\n",
    "    text = None\n",
    "\n",
    "    if url:\n",
    "        payload = record.payload.read()\n",
    "        header, html = payload.split(b'\\r\\n\\r\\n', maxsplit=1)\n",
    "        html = html.strip()\n",
    "\n",
    "        if len(html) > 0:\n",
    "            text = parser(html)\n",
    "\n",
    "    return url, text\n",
    "\n",
    "\n",
    "def process_warc(file_name, parser, limit=10000):\n",
    "    warc_file = warc.open(file_name, 'rb')\n",
    "    t0 = time()\n",
    "    n_documents = 0\n",
    "    for i, record in enumerate(warc_file):\n",
    "        url, doc = read_doc(record, parser)\n",
    "\n",
    "        if not doc or not url:\n",
    "            continue\n",
    "\n",
    "        n_documents += 1\n",
    "\n",
    "        if i > limit:\n",
    "            break\n",
    "\n",
    "    warc_file.close()\n",
    "    print('Parser: %s' % parser.__name__)\n",
    "    print('Parsing took %s seconds and produced %s documents\\n' % (time() - t0, n_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> !wget https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2018-05/segments/1516084886237.6/warc/CC-MAIN-20180116070444-20180116090444-00000.warc.gz\n",
    ">>> file_name = \"CC-MAIN-20180116070444-20180116090444-00000.warc.gz\"\n",
    ">>> process_warc(file_name, get_text_selectolax, 10000)\n",
    "Parser: get_text_selectolax\n",
    "Parsing took 16.170367002487183 seconds and produced 3317 documents\n",
    ">>> process_warc(file_name, get_text_bs, 10000)\n",
    "Parser: get_text_bs\n",
    "Parsing took 432.6902508735657 seconds and produced 3283 documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
